{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d57ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure paths are correct for the imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e760762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7692f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    'host': os.getenv('PG_HOST', 'localhost'),\n",
    "    'port': os.getenv('DB_PORT', '5432'),\n",
    "    'database': os.getenv('PG_DATABASE', 'your_database'),\n",
    "    'user': os.getenv('PG_USERNAME', 'postgres'),\n",
    "    'password': os.getenv('PG_PASSWORD', '')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aae8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVSchemaInferrer:\n",
    "    \"\"\"Infers PostgreSQL schema from CSV file.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_column_name(column_name):\n",
    "        \"\"\"Convert column name to PostgreSQL-friendly format.\"\"\"\n",
    "        # Convert to lowercase, replace spaces and special chars with underscore\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', column_name.lower())\n",
    "        # Remove consecutive underscores\n",
    "        sanitized = re.sub(r'_+', '_', sanitized)\n",
    "        # Remove leading/trailing underscores\n",
    "        sanitized = sanitized.strip('_')\n",
    "        # Ensure it doesn't start with a number\n",
    "        if sanitized and sanitized[0].isdigit():\n",
    "            sanitized = f'col_{sanitized}'\n",
    "        return sanitized or 'unnamed_column'\n",
    "    \n",
    "    @staticmethod\n",
    "    def infer_data_type(values, column_name):\n",
    "        \"\"\"Infer PostgreSQL data type from sample values.\"\"\"\n",
    "        # Remove None/empty values for analysis\n",
    "        non_empty_values = [v for v in values if v and str(v).strip()]\n",
    "        \n",
    "        if not non_empty_values:\n",
    "            return 'TEXT'\n",
    "        \n",
    "        # Check for integer\n",
    "        if all(CSVSchemaInferrer._is_integer(v) for v in non_empty_values):\n",
    "            max_val = max(int(v) for v in non_empty_values)\n",
    "            min_val = min(int(v) for v in non_empty_values)\n",
    "            if min_val >= -32768 and max_val <= 32767:\n",
    "                return 'SMALLINT'\n",
    "            elif min_val >= -2147483648 and max_val <= 2147483647:\n",
    "                return 'INTEGER'\n",
    "            else:\n",
    "                return 'BIGINT'\n",
    "        \n",
    "        # Check for numeric/decimal\n",
    "        if all(CSVSchemaInferrer._is_numeric(v) for v in non_empty_values):\n",
    "            return 'NUMERIC'\n",
    "        \n",
    "        # Check for date/timestamp\n",
    "        if all(CSVSchemaInferrer._is_date(v) for v in non_empty_values):\n",
    "            # Check if it includes time component\n",
    "            if any('T' in str(v) or ':' in str(v) for v in non_empty_values):\n",
    "                return 'TIMESTAMP'\n",
    "            return 'DATE'\n",
    "        \n",
    "        # Check for boolean\n",
    "        if all(str(v).strip().upper() in ['TRUE', 'FALSE', 'T', 'F', '1', '0', 'YES', 'NO'] \n",
    "               for v in non_empty_values):\n",
    "            return 'BOOLEAN'\n",
    "        \n",
    "        # Default to VARCHAR with appropriate length\n",
    "        max_length = max(len(str(v)) for v in non_empty_values)\n",
    "        if max_length <= 50:\n",
    "            return 'VARCHAR(50)'\n",
    "        elif max_length <= 255:\n",
    "            return 'VARCHAR(255)'\n",
    "        else:\n",
    "            return 'TEXT'\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_integer(value):\n",
    "        \"\"\"Check if value is an integer.\"\"\"\n",
    "        try:\n",
    "            int(value)\n",
    "            return '.' not in str(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_numeric(value):\n",
    "        \"\"\"Check if value is numeric.\"\"\"\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_date(value):\n",
    "        \"\"\"Check if value is a date.\"\"\"\n",
    "        date_formats = [\n",
    "            '%Y-%m-%d', '%m/%d/%Y', '%d/%m/%Y', \n",
    "            '%Y-%m-%d %H:%M:%S', '%m/%d/%Y %H:%M:%S',\n",
    "            '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d %H:%M:%S.%f'\n",
    "        ]\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                datetime.strptime(str(value).strip(), fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_csv(csv_file_path, sample_size=100):\n",
    "        \"\"\"Analyze CSV and return schema information.\"\"\"\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            original_columns = reader.fieldnames\n",
    "            \n",
    "            # Collect sample values\n",
    "            sample_data = {col: [] for col in original_columns}\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= sample_size:\n",
    "                    break\n",
    "                for col in original_columns:\n",
    "                    sample_data[col].append(row.get(col, ''))\n",
    "        \n",
    "        # Build schema\n",
    "        schema = []\n",
    "        column_mapping = {}  # Maps original column names to sanitized names\n",
    "        \n",
    "        for orig_col in original_columns:\n",
    "            sanitized_col = CSVSchemaInferrer.sanitize_column_name(orig_col)\n",
    "            data_type = CSVSchemaInferrer.infer_data_type(sample_data[orig_col], orig_col)\n",
    "            schema.append((sanitized_col, data_type))\n",
    "            column_mapping[orig_col] = sanitized_col\n",
    "        \n",
    "        return schema, column_mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1270251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PostgreSQLTableManager:\n",
    "    \"\"\"Manages PostgreSQL table creation and data loading.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_config):\n",
    "        self.db_config = db_config\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        self.conn = psycopg2.connect(**self.db_config)\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def disconnect(self):\n",
    "        \"\"\"Close database connection.\"\"\"\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "    \n",
    "    def create_table(self, table_name, schema, drop_if_exists=False):\n",
    "        \"\"\"Create table with inferred schema.\"\"\"\n",
    "        if drop_if_exists:\n",
    "            drop_sql = sql.SQL(\"DROP TABLE IF EXISTS {} CASCADE;\").format(\n",
    "                sql.Identifier(table_name)\n",
    "            )\n",
    "            self.cursor.execute(drop_sql)\n",
    "            print(f\"Dropped existing table: {table_name}\")\n",
    "        \n",
    "        # Build CREATE TABLE statement\n",
    "        column_defs = []\n",
    "        for col_name, col_type in schema:\n",
    "            column_defs.append(f\"{col_name} {col_type}\")\n",
    "        \n",
    "        create_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            {', '.join(column_defs)},\n",
    "            loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cursor.execute(create_sql)\n",
    "        self.conn.commit()\n",
    "        print(f\"Table '{table_name}' created successfully.\")\n",
    "    \n",
    "    def create_indexes(self, table_name, index_columns):\n",
    "        \"\"\"Create indexes on specified columns.\"\"\"\n",
    "        for col in index_columns:\n",
    "            index_name = f\"idx_{table_name}_{col}\"\n",
    "            index_sql = sql.SQL(\n",
    "                \"CREATE INDEX IF NOT EXISTS {} ON {} ({});\"\n",
    "            ).format(\n",
    "                sql.Identifier(index_name),\n",
    "                sql.Identifier(table_name),\n",
    "                sql.Identifier(col)\n",
    "            )\n",
    "            try:\n",
    "                self.cursor.execute(index_sql)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not create index on {col}: {e}\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "        print(f\"Indexes created for table '{table_name}'.\")\n",
    "    \n",
    "    def load_csv_data(self, table_name, csv_file_path, column_mapping):\n",
    "        \"\"\"Load CSV data into table.\"\"\"\n",
    "        with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            original_columns = reader.fieldnames\n",
    "            sanitized_columns = [column_mapping[col] for col in original_columns]\n",
    "            \n",
    "            # Prepare INSERT statement\n",
    "            insert_sql = sql.SQL(\n",
    "                \"INSERT INTO {} ({}) VALUES ({});\"\n",
    "            ).format(\n",
    "                sql.Identifier(table_name),\n",
    "                sql.SQL(', ').join(map(sql.Identifier, sanitized_columns)),\n",
    "                sql.SQL(', ').join(sql.Placeholder() * len(sanitized_columns))\n",
    "            )\n",
    "            \n",
    "            rows_inserted = 0\n",
    "            batch_size = 1000\n",
    "            batch = []\n",
    "            \n",
    "            for row in reader:\n",
    "                values = []\n",
    "                for orig_col in original_columns:\n",
    "                    value = row.get(orig_col, '').strip()\n",
    "                    values.append(None if value == '' else value)\n",
    "                \n",
    "                batch.append(values)\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    self.cursor.executemany(insert_sql, batch)\n",
    "                    rows_inserted += len(batch)\n",
    "                    batch = []\n",
    "            \n",
    "            # Insert remaining rows\n",
    "            if batch:\n",
    "                self.cursor.executemany(insert_sql, batch)\n",
    "                rows_inserted += len(batch)\n",
    "            \n",
    "            self.conn.commit()\n",
    "            print(f\"Loaded {rows_inserted} rows into '{table_name}'.\")\n",
    "            return rows_inserted\n",
    "\n",
    "\n",
    "def get_table_name_from_filename(filename):\n",
    "    \"\"\"Convert filename to table name.\"\"\"\n",
    "    # Remove extension\n",
    "    name = Path(filename).stem\n",
    "    # Sanitize\n",
    "    name = re.sub(r'[^a-zA-Z0-9_]', '_', name.lower())\n",
    "    name = re.sub(r'_+', '_', name)\n",
    "    return name.strip('_')\n",
    "\n",
    "\n",
    "def load_all_csvs(data_directory, db_config, drop_existing=False):\n",
    "    \"\"\"Load all CSV files from directory into PostgreSQL.\"\"\"\n",
    "    data_dir = Path(data_directory)\n",
    "    csv_files = list(data_dir.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {data_directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process.\\n\")\n",
    "    \n",
    "    manager = PostgreSQLTableManager(db_config)\n",
    "    manager.connect()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        for csv_file in csv_files:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing: {csv_file.name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Generate table name\n",
    "            table_name = get_table_name_from_filename(csv_file.name)\n",
    "            print(f\"Table name: {table_name}\")\n",
    "            \n",
    "            # Analyze CSV and infer schema\n",
    "            schema, column_mapping = CSVSchemaInferrer.analyze_csv(str(csv_file))\n",
    "            print(f\"Detected {len(schema)} columns:\")\n",
    "            for col_name, col_type in schema:\n",
    "                print(f\"  - {col_name}: {col_type}\")\n",
    "            \n",
    "            # Create table\n",
    "            manager.create_table(table_name, schema, drop_if_exists=drop_existing)\n",
    "            \n",
    "            # Load data\n",
    "            rows_loaded = manager.load_csv_data(table_name, str(csv_file), column_mapping)\n",
    "            \n",
    "            # Create indexes on common column patterns\n",
    "            index_candidates = [col for col, _ in schema \n",
    "                              if any(keyword in col for keyword in \n",
    "                                   ['id', 'date', 'status', 'category', 'priority', 'region'])]\n",
    "            if index_candidates:\n",
    "                print(f\"Creating indexes on: {', '.join(index_candidates)}\")\n",
    "                manager.create_indexes(table_name, index_candidates)\n",
    "            \n",
    "            results.append({\n",
    "                'file': csv_file.name,\n",
    "                'table': table_name,\n",
    "                'rows': rows_loaded,\n",
    "                'columns': len(schema)\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during processing: {e}\")\n",
    "        manager.conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        manager.disconnect()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for result in results:\n",
    "        print(f\"✓ {result['file']} → {result['table']}: \"\n",
    "              f\"{result['rows']} rows, {result['columns']} columns\")\n",
    "    print(f\"\\nTotal: {len(results)} tables created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set drop_existing=True to recreate tables\n",
    "load_all_csvs(DATA_DIR, DB_CONFIG, drop_existing=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ticketier (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
